This is the model description. For operation instructions please open the 'Run Instructions.md' file

<img src="https://media.githubusercontent.com/media/Christoper-Harvey/1st-Capstone/master/model.png" >

This problem is a reinforcement learning problem. One that learns from self play and no data. I will explain what the model does, how it does it, and how to improve it in the future.

The model I used is a Convolutional Residual Network with two heads. A policy head and a value head. The policy head makes decisions for the agent. The value head decides how good of a decision that the policy head made was. Together they update their weights and try to create a better model.

The Residual part of the network is as many layers as you can compute. All consisting of two conv2d layers that are batch normalized and activated with Leaky ReLU. My model used 5 residual layers. The more layers you have the better the model will generalize and will become slightly more stable. These layers allow the model to randomly select positions and learn the correct weights over time as it becomes more familiar with its environment. It can effectively turn a 100 layer model into a 10 layer model just by letting the model go straight from input to the two output heads. This in turn leads to the model learning quickly and exploring the environment heavily in the first moments of its life. As it learns it will naturally start using more and more residual layers until it creates a complex representation of its environment. Even though the training will start at the same speed no matter how many layers you use, be warned that as you add more residual layers, when they start being developed, each layer will drastically increase computation time!

The policy head is powered by a Neural Network. In this case it is a Deep Convolutional network with Batch normalization and a Leaky ReLU activation function. It is flattened and sent through a dense layer as the classifier with 25 outputs for the various decisions.

The value head is also powered by a convnet but has an extra dense layer and only has 1 output in the final dense layer that is a number that tells how good the decision was.

The model then takes state inputs as the current state of the board. This can be custom for each game; Say you want to take every tenth of a second in a fighting game or every board state in chess. We can easily create a way for the agent to see what is happening in the environment. The model then takes every state and processes it and sends the data to the policy head and the Monte Carlo Search Tree to see what action it wants to take. After it takes this action the value head judges it and then they both do back-prop to update their weights. This leads to both agents learning how to do their job the best way possible while searching for overall success based on a value function.

In reinforcement learning a value function is a way to determine the overall success of the agent as a discounted cumulative reward of each action taken. This doesn’t try to update like normal Deep or classical machine learners as this only tries to increase long term success and not immediate loss or accuracy. That creates a sense of a ‘goal’ in which the agent will try to actually ‘do’ something. Every reinforcement learner in that sense is a type of AI.

This model after each game does a massive update to both the MCTS and the policy head to make them better after each game. This method doesn’t allow the model to stall or get stuck in local maxima very long or often as exploration of the search tree and available actions is taken at a constant or decreasing rate over time. I use an epsilon-greedy approach which means that 20% of the time the model will select a random action and 80% of the time the model will select only the best action. This can also be done with epsilon-decay which means you start with 100% epsilon then after each action you multiply it by 99.5% with a cap of 1% epsilon. This allows the tree to fill up with very different paths to take and allows for great creativity in the model. I choose to do both I make the first 10 games the model plays 100% epsilon to fill up the memory, then I drop it down to a constant rate of 20%. This in my opinion is one of the best ways to encourage diverse behaviour while still profiting early with high potential paths.


So the major parts of the model is the ConvResNet, the policy head with a MCTS, the value head, the epsilon-greedy exploration technique, and the environment setup. If I wanted to improve my model even further I could do the following: Create a better MCTS policy to make more disparate leafs for further creativity, Use a more tuned epsilon-decay method, Use a Proximal Policy Optimization technique instead of A2C, Use curiosity learning instead of e-Greedy techniques for the exploration/exploitation problem, Add more layers to my ConvResNet and possible more dense layers for the value head, Fiddle with the value function and rewards to create a stronger goal for the agent, or try to create a method that allows more data to updated at one time and it still be stable.